<!DOCTYPE html>
<html>
<head>
<style>
    /* Apply margins of 100px for screens wider than 768px */
    @media screen and (min-width: 1680px) {
        body {
            margin-left: 600px;
            margin-right: 600px;
        }
    }
</style>
</head>
<body>

<h2 style="text-align: right;"><img style="font-size: 14px; float: left;" src="https://github.com/YuanGongND/yuangongnd.github.io/raw/master/gcs-42018-1136.jpg" alt="" width="194" height="292" />Yuan Gong</h2>
<h2 style="text-align: right; padding-left: 30px;"><img class="alignnone  wp-image-112" src="https://github.com/YuanGongND/yuangongnd.github.io/raw/master/name.png" alt="" width="65" height="46" /></h2>
<p style="text-align: right; padding-left: 30px;">Research Scientist</p>
<p style="text-align: right; padding-left: 30px;">Computer Science & Artificial Intelligence Lab</p>
<p style="text-align: right; padding-left: 30px;">Massachusetts Institute of Technology</p>
<p style="text-align: right; padding-left: 30px;">G442, 32 Vassar St, Cambridge MA 02139</p>
<p style="text-align: right; padding-left: 30px;">Phone: (574) 401-0833</p>
<p style="text-align: right; padding-left: 30px;">Email: yuangong@mit.edu</p>
<h3>&nbsp;</h3>

<h3>Research Interest</h3>
<hr />
<strong>AI for Audio, Speech, and Natural Lanaguage Processing</strong>, including the following topics: large language models, foundation audio and speech AI models, audio-visual multi-modal AI, speech AI system for health applications, and secure and trustworthy speech AI.

<h3>Bio</h3>
<hr />
  
<p>I am a Research Scientist at the MIT CSAIL Spoken Language Systems Group (SLS), working with <a href="https://www.csail.mit.edu/person/jim-glass">Dr. James Glass</a>. Before I joined MIT, I got my Ph.D. in computer science from the University of Notre Dame, supervised by <a href="https://www.cis.fiu.edu/faculty-staff/christian-poellabauer/">Dr. Christian Poellabauer</a>. During the 2019 Summer, I was an applied scientist intern working on clinical text mining in the <a href="https://aws.amazon.com/comprehend/medical/">AWS Comprehend Medical</a> team, supervised by <a href="https://scholar.google.com/citations?user=z8dFyzAAAAAJ&amp;hl=en">Mohammed Khalilia</a> and <a href="https://scholar.google.com/citations?hl=en&amp;user=8F3svqgAAAAJ">Parminder Bhatia</a>. Before coming to Notre Dame, I got my B.Sc. degree in Electrical Engineering (Biomedical Engineering Major) from Fudan University in 2015. My research advisors were <a href="http://www.it.fudan.edu.cn/En/Data/View/1767">Dr. Yuanyuan Wang</a> (on ultrasound image denoising) and <a href="http://medianet.azurewebsites.net/yuedong-xu/">Dr. Yuedong Xu</a> (on network science). <br>

<br><a href="https://www.dropbox.com/scl/fi/80kmgelajn9r1letxixhn/gong_cv.pdf?rlkey=qvoudy1k8eqtiudujve19ft3z&dl=0" target="_blank">Curriculum Vitae</a> | 
<a href="https://scholar.google.com/citations?user=MuhvvOkAAAAJ&hl=en;hl=en" target="_blank" rel="noopener">Google Scholar</a></p>

<h3 style="text-align: left;"><strong>Education</strong></h3>
<hr />
<p>2020.7&nbsp; &nbsp;Ph.D., Computer Science and Engineering, <a href="https://en.wikipedia.org/wiki/University_of_Notre_Dame">University of Notre Dame</a>, IN, USA&nbsp; (GPA: 4.0/4.0)</p>
<p>2015.7&nbsp; &nbsp;B.Sc., Electrical Engineering (Biomedical Engineering Major), <a href="https://en.wikipedia.org/wiki/Fudan_University">Fudan University</a>, Shanghai, China.&nbsp;(GPA Rank: 1/15, First Prize Scholarship)</p>

<h3 style="text-align: left;"><strong>Employment</strong></h3>
<hr />
<p>2023.8 - &emsp;&emsp; &nbsp; &nbsp; &nbsp; Research Scientist, Massachusetts Institute of Technology, Cambridge, USA</p>
<p>2020.8 - 2023.7&nbsp; &nbsp; Postdoc Research Associate, Massachusetts Institute of Technology, Cambridge, USA</p>
<p>2015.8 - 2020.7&nbsp; &nbsp; Graduate Research Assistant, University of Notre Dame, Notre Dame, USA</p>
<p>2019.5 - 2019.8&nbsp; &nbsp; Applied Scientist Intern, Amazon Web Service, Seattle, USA</p>
<p>2014.6 - 2015.7&nbsp; &nbsp; Undergraduate Research Assistant, Fudan University, Shanghai, China</p>
<p>2012.7 - 2012.8&nbsp; &nbsp; Intern, Philips Healthcare, Shanghai, China</p>

<h3>Publications</h3>
<hr />
<ul>
      <li>
    <strong>Search Challenges Large Language Models</strong><br>
    Hongyin Luo, Yung-Sung Chuang, <u>Yuan Gong</u>, Tianhua Zhang, Yoon Kim, Xixin Wu, Helen Meng, and James Glass<br>
    Proceedings of findings of the 2023 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP 2023)<br>
    <a href="https://arxiv.org/pdf/2305.15225.pdf">Paper</a> | <a href="https://github.com/luohongyin/SAIL">Code</a>
  </li><br>
  <li>
    <strong>Joint Audio and Speech Understanding</strong> <span style="color: #ff0000;">(top 3% paper, best paper finalist)</span><br> 
    <u>Yuan Gong</u>, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, and James Glass<br>
    Proceedings of the 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2023), Taipei, December 2023 <br>
    <a href="https://arxiv.org/pdf/2309.14405.pdf">Paper</a> | <a href="https://huggingface.co/spaces/yuangongfdu/ltu-2">Interactive Demo</a> | <a href="https://github.com/YuanGongND/ltu">Code</a> | <a href="https://github.com/YuanGongND/ltu/blob/main/ltu_as_poster_asru.pdf">Poster</a>
  </li><br>
  <li>
    <strong>Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers</strong><br>
    <u>Yuan Gong</u>, Sameer Khurana, Leonid Karlinsky, and James Glass<br>
    Proceedings of the 24th Conference of the International Speech Communication Association (Interspeech 2023), Dublin, Ireland, August 2023<br>
    <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/gong23d_interspeech.pdf">Paper</a> | <a href="https://github.com/YuanGongND/whisper-at">Code</a> | <a href="https://huggingface.co/spaces/yuangongfdu/whisper-at">Interactive Demo</a> | <a href="https://github.com/YuanGongND/whisper-at/raw/main/poster.png?raw=true">Poster</a>
  </li><br>
  <li>
    <strong>Contrastive Audio-Visual Masked Autoencoder</strong> <span style="color: #ff0000;">(notable-top-25% paper)</span><br> 
    <u>Yuan Gong</u>, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James Glass<br>
    Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda, May 2023 (ICLR 2023)<br>
    <a href="https://openreview.net/forum?id=QPtMRyk5rb">Paper</a> | <a href="https://github.com/YuanGongND/cav-mae">Code</a> | <a href="https://recorder-v3.slideslive.com/?share=80147&s=29e0bafa-9193-4971-ac6f-e5a4cf7d69a0">Video</a> | <a href="https://docs.google.com/presentation/d/1l-cofX9liikkVG6TkH2XwboYLVJoUDhpcyyq-fqtfcM/edit?usp=sharing">Slides</a> | <a href="https://iclr.cc/media/PosterPDFs/ICLR%202023/10966.png?t=1680929792.682869">Poster</a> | <a href="https://news.mit.edu/2023/scaling-audio-visual-learning-without-labels-0605">MIT News</a>
  </li><br>
  <li>
    <strong>Improving Computational Efficiency of Voice Anti-Spoofing Models</strong><br> 
    Jian Yang, Bryan Ning Xia, John Bailey, <u>Yuan Gong</u>, John Michael Templeton, and Christian Poellabauer<br>
    Proceedings of the 2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS 2023)<br>
    <a href="https://ieeexplore.ieee.org/document/10298569">Paper</a> | <a href="https://github.com/YuanGongND/efficient-voice-antispoof/tree/master">Code</a>
  </li><br>
  <li>
    <strong>UAVM: Towards Unifying Audio and Visual Models</strong><br>
    <u>Yuan Gong</u>, Alexander H. Liu, Andrew Rouditchenko, and James Glass<br>
    IEEE Signal Processing Letters, 2022<br>
    <a href="https://ieeexplore.ieee.org/document/9964072">Paper</a> | <a href="https://github.com/YuanGongND/uavm">Code</a>
  </li><br>
  <li>
    <strong>Detecting Dementia from Long Neuropsychological Interviews</strong><br>
    Nauman Dawalatabad, <u>Yuan Gong</u>, Sameer Khurana, Rhoda Au, and James Glass<br>
    Proceedings of findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP 2022), Abu Dhabi, December 2022<br>
    <a href="https://aclanthology.org/2022.findings-emnlp.386">Paper</a>
  </li><br>
  <li>
    <strong>Vocalsound: A Dataset For Improving Human Vocal Sounds Recognition</strong><br>
    <u>Yuan Gong</u>, Jin Yu, and James Glass<br>
    Proceedings of the 47th International Conference on Acoustics, Speech, & Signal Processing (ICASSP 2022), Singapore, May 2022<br>
    <a href="https://ieeexplore.ieee.org/document/9746828">Paper</a> | <a href="https://github.com/YuanGongND/vocalsound">Dataset & Code</a> | <a href="https://youtu.be/SnTwSaJ0YCo">Video</a> | <a href="https://drive.google.com/file/d/1kdaT3SZVQuDpRgOsaN8NvdqWteLe5cfl/view?usp=sharing">Slides</a>
  </li><br>
  <li>
    <strong>Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment</strong><br>
    <u>Yuan Gong</u>, Ziyi Chen, Iek-Heng Chu, Peng Chang, and James Glass<br>
    Proceedings of the 47th International Conference on Acoustics, Speech, & Signal Processing (ICASSP 2022), Singapore, May 2022<br>
    <a href="https://ieeexplore.ieee.org/document/9746743">Paper</a> | <a href="https://github.com/YuanGongND/gopt">Code</a> | <a href="https://youtu.be/vIGZWcKfdKY">Video</a> | <a href="https://drive.google.com/file/d/1XSt61-TMmMlvOBbp0ZAeRGva-gqKLquY/view?usp=sharing">Slides</a> | <a href="https://nanyang2015.github.io/blog/yu-yin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/#2022-Multi-Aspect-Multi-Granularity">Blog in Chinese</a>
  </li><br>
  <li>
    <strong>SSAST: Self-Supervised Audio Spectrogram Transformer</strong><br>
    <u>Yuan Gong</u>, Cheng-I Jeff Lai, Yu-An Chung, and James Glass<br>
    Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI 2022), Vancouver, Canada, February-March 2022<br>
    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21315">Paper</a> | <a href="https://github.com/YuanGongND/ssast">Code</a> | <a href="https://drive.google.com/file/d/1X4d21qJUSTSBpbVjB6p3IGaDH1ulxb-U/view?usp=sharing">Slides</a>
  </li><br>
  <li>
    <strong>AST: Audio Spectrogram Transformer</strong><br>
    <u>Yuan Gong</u>, Yu-An Chung, and James Glass<br>
    Proceedings of the 22nd Conference of the International Speech Communication Association (Interspeech 2021), Brno, Czech Republic, August-September 2021<br>
    <a href="https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html">Paper</a> | <a href="https://github.com/YuanGongND/ast">Code</a> | <a href="https://www.youtube.com/watch?v=CSRDbqGY0Vw">Talk</a> | <a href="https://blog.csdn.net/aidanmo/article/details/122297386">Blog in Chinese</a>
  </li><br>
  <li>
    <strong>PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation</strong><br>
    <u>Yuan Gong</u>, Yu-An Chung, and James Glass<br>
    IEEE Transactions on Audio, Speech and Language Processing, 2021<br>
    <a href="https://ieeexplore.ieee.org/document/9576629">Paper</a> | <a href="https://github.com/YuanGongND/psla">Code</a> | <a href="https://youtu.be/DIyqRNDpSfA">Video</a> | <a href="https://drive.google.com/file/d/1_HTbloecpdQ1ZZNs-L_xuc5QDDW-TyK5/view?usp=sharing">Slides</a> | <a href="https://blog.csdn.net/rush9838465/article/details/122164791">Blog in Chinese</a>
  </li><br>
  <li>
    <strong>Detecting Replay Attacks Using Multi-Channel Audio: A Neural Network-Based Method</strong><br>
    <u>Yuan Gong</u>, Jian Yang, and Christian Poellabauer<br>
    IEEE Signal Processing Letters, 2020<br>
    <a href="https://ieeexplore.ieee.org/document/9099075">Paper</a> | <a href="https://github.com/YuanGongND/multichannel-antispoof">Code</a>
  </li><br>
  <li>
    <strong>Second-order Non-local Attention Networks for Person Re-identification</strong><br>
    Bryan Xia, <u>Yuan Gong</u>, Yizhe Zhang, and Christian Poellabauer<br>
    Proceedings of the 2019 International Conference on Computer Vision (ICCV), Seoul, Korea, October-November 2019<br>
    <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.pdf">Paper</a> | <a href="https://blog.csdn.net/weixin_38208912/article/details/104419781">Blog</a>
  </li><br>
  <li>
    <strong>ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems </strong>(<span style="color: #ff0000;">best student paper award nomination</span>)<br>
    <u>Yuan Gong</u>, Jian Yang, Jacob Huber, Mitchell MacKnight, Christian Poellabauer<br>
    Proceedings of the 20th Conference of the International Speech Communication Association (Interspeech 2019), Graz, Austria, September 2019<br>
    <a href="https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html" target="_blank" rel="noopener">Paper</a> | <a href="https://github.com/YuanGongND/ReMASC" target="_blank" rel="noopener">Dataset</a>
  </li><br>
  <li>
    <strong>Real-time Adversarial Attacks</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span>, Boyang Li, Christian Poellabauer, and Yiyu Shi<br>
    Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), Macao, China, August 2019.<br>
    <a href="https://www.ijcai.org/proceedings/2019/649" target="_blank" rel="noopener">Paper</a>  | 
    <a href="https://github.com/YuanGongND/realtime-adversarial-attack">Code</a>  | 
    <a href="https://medium.com/ai%C2%B3-theory-practice-business/how-about-real-time-adversarial-attacks-6aba92d59c1e">Media</a>
  </li><br>
  <li>
    <strong>Deep Obfuscation: Precise Masking of Sensitive Information to Protect Against Machine Learning Adversaries (Poster)</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 2018 Annual Computer Security Applications Conference Poster Session, San Juan, Puerto Rico, December 2018.
  </li><br>
  <li>
    <strong>Crafting Adversarial Examples For Speech Paralinguistics Applications</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security (DYNAMICS) Workshop, San Juan, Puerto Rico, December 2018.<br>
    <a href="https://arxiv.org/pdf/1711.03280.pdf">Paper</a>
  </li><br>
  <li>
    <strong>Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span>, Kevin Shin, and Christian Poellabauer<br>
    Proceedings of the 19th Conference of the International Speech Communication Association (Interspeech 2018), Hyderabad, India, September 2018.<br>
    <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1371.pdf">Paper</a>
  </li><br>
  <li>
    <strong>Improving LIWC Using Soft Word Matching (Poster)</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span>, Hasini Yatawatte, Christian Poellabauer, Sandra Schneider, and Susan Latham<br>
    Proceedings of the 9th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB), Washington, DC, August-September 2018.<br>
    <a href="https://dl.acm.org/authorize?N667629">Paper</a>
  </li><br>
  <li>
    <strong>Automatic Autism Spectrum Disorder Detection Using Everyday Vocalizations Captured by Smart Devices</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 9th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB), Washington, DC, August-September 2018.<br>
    <a href="https://dl.acm.org/authorize?N667607">Paper</a>
  </li><br>
  <li>
    <strong>Protecting Voice Controlled Systems Using Sound Source Identification Based on Acoustic Cues</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 27th International Conference on Computer Communications and Networks (ICCCN), Hangzhou, China, July-August 2018.<br>
    <a href="https://ieeexplore.ieee.org/document/8487334/authors#authors" target="_blank" rel="noopener noreferrer">Paper</a>
  </li><br>
  <li>
    <strong>An Overview of Vulnerabilities of Voice Controlled Systems</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 1st International Workshop on Security and Privacy for the Internet-of-Things (IoTSec), Orlando, FL, April 2018.<br>
    <a href="https://arxiv.org/pdf/1803.09156.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
  </li><br>
  <li>
    <strong>Topic Modeling Based Multi-modal Depression Detection </strong>(<span style="color: #ff0000;">depression challenge winner</span>)<br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 7th Audio/Visual Emotion Challenge and Workshop (AVEC) in conjunction with ACM Multimedia (ACM-MM), Mountain View, CA, October 2017.<br>
    <a href="https://arxiv.org/pdf/1803.10384.pdf" target="_blank" rel="noopener noreferrer">Paper</a>
  </li><br>
  <li>
    <strong>Continuous Assessment of Children's Emotional States using Acoustic Analysis</strong><br>
    <span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer<br>
    Proceedings of the 5th IEEE International Conference on Healthcare Informatics (ICHI), Park City, UT, August 2017.<br>
    <a href="https://ieeexplore.ieee.org/document/8031145/" target="_blank" rel="noopener noreferrer">Paper</a>
  </li><br>
  <li>
    <strong>A Smart Low-Power-Consumption ECG Monitor Based on MSP430F5529 and CC2540</strong> (<span style="color: #ff0000;">winner of the 2014 TI national (China) biomedical device design contest</span>)<br>
    <span style="text-decoration: underline;">Yuan Gong</span>, Jin Cao, Zehui Luo, Guohui Zhou<br>
    Chinese Journal of Medical Instrumentation 39.4, 2015<br>
    <a href="http://www.cqvip.com/qk/93920x/201504/665744394.html" target="_blank" rel="noopener noreferrer">Paper</a>
  </li>
</ul>
<h3>Preprint</h3>
<hr />
<ul>
<li>
  <strong>Listen, Think, and Understand</strong><br> 
  <u>Yuan Gong</u>, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James Glass<br>
</span>
  <a href="[Paper Link]">Paper</a> | <a href="[Demo Link]">Interactive Demo</a> | <a href="[Code Link]">Code</a>
</li><br>

<li>
  <strong>CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification</strong><br> 
  <u>Yuan Gong</u>, Sameer Khurana, Andrew Rouditchenko, and James Glass<br>
  <a href="[Paper Link]">Paper</a>
</li>
</ul>
<h3>Awards</h3>
<hr />
<ul>
<li>ASRU Best Paper Finalist (top 3% paper, 12/435)</li>
<li>ICASSP 2023 Outstanding Reviewer</li>
<li>INTERSPEECH 2019 Best Student Paper Award Nomination</li>
<li><a href="http://sspnet.eu/avec2017/">Depression Detection Challenge Winner</a>, the 7th ACM Multimedia Audio/Visual Emotion Challenge and Workshop (AVEC 2017)</li>
<li>IJCAI, ISCA, ICHI, NSF Travel Grant</li>
<li>Outstanding Graduate of Fudan University (2015), Fudan First Prize Scholarship (Top 3%, 2014),&nbsp;&nbsp;Outstanding Student of Dept. of Information Technology (2013),&nbsp;&nbsp;Outstanding Student of Fudan University (2012)</li>
</ul>
<h3>Invited Talks and Guest Lectures</h3>
<hr />
<ul>
<li>
  <strong>How We Evaluate Our Audio and Speech Large Language Model?</strong><br>
  <i>ASRU SPARKS Workshop,</i> 12/16/2023. [<a href="https://docs.google.com/presentation/d/1jUyqYuYN1ieF__WB8vL1W7cUqk2jjK7snCREbwA40sg/edit?usp=sharing">slides</a>]
</li><br>

<li>
  <strong>Recent Progress of MIT SLS's Research on Audio Classification and Understanding (two-hour tutorial talk)</strong><br>
  <i>Amazon,</i> 12/15/2023. [<a href="https://docs.google.com/presentation/d/166CJ8ot3CSafP9s1DbRKObJbKJdmeezhyPfTstj79fA/edit?usp=sharing">slides</a>]
</li><br>

<li>
  <strong>Audio Large Language Models: From Sound Perception to Understanding</strong><br>
  <i>MIT Embodied Intelligence Seminar,</i> 10/19/2023. [<a href="https://www.youtube.com/live/uqsW2eK-Rms?si=D-38elgK3-5TvAlY&t=48">video</a>][<a href="https://docs.google.com/presentation/d/1k8ySUdLfaLwvHNbb_oDk1QIrE6dQbVKe9Ysi0g4o5TE/edit?usp=sharing">slides</a>]; <i>SANE Workshop 2023,</i> 10/26/2023.
</li><br>

<li>
  <strong>Contrastive Audio-Visual Masked Autoencoder</strong><br>
  <i>IBM,</i> 7/28/2023; <i>Hong Kong University of Science and Technology,</i> 10/11/2023.
</li><br>

<li>
  <strong>Large Language Models that Listen</strong><br>
  <i>Takeda,</i> 5/30/2023; <i>Signify,</i> 7/21/2023.
</li><br>

<li>
  <strong>Introduction of Audio Spectrogram Transformer - Architecture, Training, and Pre-training</strong><br>
  <i>Mitsubishi Electric Research Laboratories,</i> 6/8/2022; <i>ByteDance,</i> 6/14/2022; <i>Adobe,</i> 7/12/2022.
</li><br>

<li>
  <strong>Introduction of Audio Spectrogram Transformer - Architecture, Training, and Pre-training</strong><br>
  <i>AI Time.</i> 5/26/2022. [<a href="https://www.bilibili.com/video/BV1Va411j73F?spm_id_from=333.999.0.0">video in Mandarin</a>][<a href="https://drive.google.com/file/d/15LWH0QRHkfPWtCxQJ-qdrKOJ96LKEj0c/view?usp=sharing">slides</a>]
</li><br>

<li>
  <strong>General Audio Processing</strong><br>
  <i>MIT 6.345/HST.728 Spoken Language Processing (Guest Lecture).</i> 4/19/2022.
</li><br>

<li>
  <strong>Audio Spectrogram Transformer</strong><br>
  <i>MIT Embodied Intelligence Seminar.</i> 10/14/2021. [<a href="https://youtu.be/wcVejqmb1mQ?t=1556">video</a>]
</li><br>

<li>
  <strong>Audio Spectrogram Transformer for Audio Scene Analysis</strong><br>
  <i>ISCA SIGML Seminar.</i> 6/16/2021. [<a href="https://youtu.be/CSRDbqGY0Vw">video</a>][<a href="https://drive.google.com/file/d/1mKktIq5OAltcIXwI45vLRGqx3VxjLSn7/view?usp=sharing">slides</a>]
</li><br>

<li>
  <strong>Win the cat and mouse game: ensuring the security of the speech processing systems to real-world threats</strong><br>
  <i>University of Notre Dame CSE60641 (Guest Lecture).</i> 10/31/2019. [<a href="https://drive.google.com/file/d/1EMRhJR0dv1NSj8MS55nHNVD7z7waCq7e/view?usp=sharing">slides</a>]
</li><br>

<li>
  <strong>Speech Processing: Machine Learning Approaches, Novel Applications, and New Security Concerns</strong><br>
  <i>University of Notre Dame CSE60641 (Guest Lecture, Sole Instructor).</i> 9/20/2018. [<a href="https://drive.google.com/file/d/1mKktIq5OAltcIXwI45vLRGqx3VxjLSn7/view?usp=sharing">slides</a>]
</li>
</ul>
<h3>Contact</h3>
<hr />
<p>Please feel free to reach out (yuangong@mit.edu) if you have any questions about my work. I do not use WeChat. As a research scientist, I am not involved in student/researcher recruitment at CSAIL, please contact a PI for such inquiries.</p>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=rra7QS5zkujIOvkpex4FoZ6CWhS0vZIMatAVAMCJnvU'></script>
<p>&nbsp;</p>

</body>
</html>
