<h2 style="text-align: right;"><img style="font-size: 14px; float: left;" src="https://github.com/YuanGongND/yuangongnd.github.io/raw/master/gcs-42018-1136.jpg" alt="" width="194" height="292" />Yuan Gong</h2>
<h2 style="text-align: right; padding-left: 30px;"><img class="alignnone  wp-image-112" src="https://github.com/YuanGongND/yuangongnd.github.io/raw/master/name.png" alt="" width="65" height="46" /></h2>
<p style="text-align: right; padding-left: 30px;">Postdoc Associate</p>
<p style="text-align: right; padding-left: 30px;">Computer Science & Artificial Intelligence Lab</p>
<p style="text-align: right; padding-left: 30px;">Massachusetts Institute of Technology</p>
<p style="text-align: right; padding-left: 30px;">G442, 32 Vassar St, Cambridge MA 02139</p>
<p style="text-align: right; padding-left: 30px;">Phone: (574) 401-0833</p>
<p style="text-align: right; padding-left: 30px;">Email: yuangong@mit.edu</p>
<h3>&nbsp;</h3>
<h3>Bio</h3>
<hr />
<p>I am a postdoc associate at the MIT Computer Science and Artificial Intelligence Lab (CSAIL). Before I joined MIT, I got my Ph.D. in computer science from the University of Notre Dame, supervised by <a href="https://www3.nd.edu/~cpoellab/">Dr. Christian Poellabauer</a>. During the 2019 Summer, I was an applied scientist intern working on clinical text mining in the <a href="https://aws.amazon.com/comprehend/medical/">AWS Comprehend Medical</a> team, supervised by <a href="https://scholar.google.com/citations?user=z8dFyzAAAAAJ&amp;hl=en">Mohammed Khalilia</a> and <a href="https://scholar.google.com/citations?hl=en&amp;user=8F3svqgAAAAJ">Parminder Bhatia</a>. Before coming to Notre Dame, I got my B.Sc. degree in Electrical Engineering (Biomedical Engineering Major) from Fudan University in 2015. My research advisors were <a href="http://www.it.fudan.edu.cn/En/Data/View/1767">Dr. Yuanyuan Wang</a> (on ultrasound image denoising) and <a href="http://medianet.azurewebsites.net/">Dr. Yuedong Xu</a> (on network science). My current research interest is computational speech and audio signal analysis, which includes the following topics: speech-based healthcare applications, audio-visual multi-modality learning, and general audio event recognition. <a href="https://drive.google.com/file/d/1RCWT_09Ia8DHpOsvsgNG1WVz-CiRmqSP/view?usp=sharing" target="_blank">[CV]</a>&nbsp;[<a href="https://scholar.google.com/citations?user=MuhvvOkAAAAJ&hl=en;hl=en" target="_blank" rel="noopener">Google Scholar</a>]</p>
<h3 style="text-align: left;"><strong>Education</strong></h3>
<hr />
<p>2020.7&nbsp; &nbsp;Ph.D., Computer Science and Engineering, <a href="https://en.wikipedia.org/wiki/University_of_Notre_Dame">University of Notre Dame</a>, IN, USA&nbsp; (GPA: 4.0/4.0)</p>
<p>2015.7&nbsp; &nbsp;B.Sc., Electrical Engineering (Biomedical Engineering Major), <a href="https://en.wikipedia.org/wiki/Fudan_University">Fudan University</a>, Shanghai, China.&nbsp;(GPA Rank: 1/15, First Prize Scholarship)</p>
<h3 style="text-align: left;"><strong>Experience</strong></h3>
<hr />
<p>2020.8 - &emsp;&emsp; &nbsp; &nbsp; &nbsp; Postdoc Research Associate, Massachusetts Institute of Technology, Cambridge, USA</p>
<p>2015.8 - 2020.7&nbsp; &nbsp; Graduate Research Assistant, University of Notre Dame, Notre Dame, USA</p>
<p>2019.5 - 2019.8&nbsp; &nbsp; Applied Scientist Intern, Amazon Web Service, Seattle, USA</p>
<p>2014.6 - 2015.7&nbsp; &nbsp; Undergraduate Research Assistant, Fudan University, Shanghai, China</p>
<p>2012.7 - 2012.8&nbsp; &nbsp; Intern, Philips Healthcare, Shanghai, China</p>
<h3>Publications</h3>
<hr />
<ul>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James Glass,&nbsp;<strong>"Contrastive Audio-Visual Masked Autoencoder"</strong>, Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda, May 2023 (ICLR 2023, <em>notable-top-25% paper</em>). [<a href="https://openreview.net/forum?id=QPtMRyk5rb">Paper</a>][<a href="https://github.com/YuanGongND/cav-mae">Code</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Alexander H. Liu, Andrew Rouditchenko, and James Glass,&nbsp;<strong>"UAVM: Towards Unifying Audio and Visual Models"</strong>, IEEE Signal Processing Letters, 2022. [<a href="https://ieeexplore.ieee.org/document/9964072">Paper</a>][<a href="https://github.com/YuanGongND/uavm">Code</a>]</li>
<li>Nauman Dawalatabad, <span style="text-decoration: underline;">Yuan Gong</span>, Sameer Khurana, Rhoda Au, and James Glass,&nbsp;<strong>"Detecting Dementia from Long Neuropsychological Interviews"</strong>, Proceedings of findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP 2022), Abu Dhabi, December 2022. [<a href="https://aclanthology.org/2022.findings-emnlp.386">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Jin Yu, and James Glass,&nbsp;<strong>"Vocalsound: A Dataset For Improving Human Vocal Sounds Recognition"</strong>, Proceedings of the 47th International Conference on Acoustics, Speech, & Signal Processing (ICASSP 2022), Singapore, May 2022. &nbsp;[<a href="https://ieeexplore.ieee.org/document/9746828">Paper</a>][<a href="https://github.com/YuanGongND/vocalsound">Dataset&Code</a>][<a href="https://youtu.be/SnTwSaJ0YCo">Video</a>][<a href="https://drive.google.com/file/d/1kdaT3SZVQuDpRgOsaN8NvdqWteLe5cfl/view?usp=sharing">Slides</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Ziyi Chen, Iek-Heng Chu, Peng Chang, and James Glass,&nbsp;<strong>"Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment"</strong>, Proceedings of the 47th International Conference on Acoustics, Speech, & Signal Processing (ICASSP 2022), Singapore, May 2022. &nbsp;[<a href="https://ieeexplore.ieee.org/document/9746743">Paper</a>][<a href="https://github.com/YuanGongND/gopt">Code</a>][<a href="https://youtu.be/vIGZWcKfdKY">Video</a>][<a href="https://drive.google.com/file/d/1XSt61-TMmMlvOBbp0ZAeRGva-gqKLquY/view?usp=sharing">Slides</a>][<a href="https://nanyang2015.github.io/blog/yu-yin/yu-yin-ping-ce/ping-fen/duo-wei-du-duo-li-du/#2022-Multi-Aspect-Multi-Granularity">Blog in Chinese</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Cheng-I Jeff Lai, Yu-An Chung, and James Glass,&nbsp;<strong>"SSAST: Self-Supervised Audio Spectrogram Transformer"</strong>, Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI 2022), Vancouver, Canada, February-March 2022. &nbsp;[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/21315">Paper</a>][<a href="https://github.com/YuanGongND/ssast">Code</a>][<a href="https://drive.google.com/file/d/1X4d21qJUSTSBpbVjB6p3IGaDH1ulxb-U/view?usp=sharing">Slides</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Yu-An Chung, and James Glass,&nbsp;<strong>"AST: Audio Spectrogram Transformer"</strong>, Proceedings of the 22nd Conference of the International Speech Communication Association (Interspeech 2021), Brno, Czech Republic, August-September 2021.&nbsp;[<a href="https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html">Paper</a>][<a href="https://github.com/YuanGongND/ast">Code</a>][<a href="https://www.youtube.com/watch?v=CSRDbqGY0Vw">Talk</a>][<a href="https://blog.csdn.net/aidanmo/article/details/122297386">Blog in Chinese</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Yu-An Chung, and James Glass,&nbsp;<strong>"PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation"</strong>, IEEE Transactions on Audio, Speech and Language Processing, 2021.&nbsp;[<a href="https://ieeexplore.ieee.org/document/9576629">Paper</a>][<a href="https://github.com/YuanGongND/psla">Code</a>][<a href="https://youtu.be/DIyqRNDpSfA">Video</a>][<a href="https://drive.google.com/file/d/1_HTbloecpdQ1ZZNs-L_xuc5QDDW-TyK5/view?usp=sharing">Slides</a>][<a href="https://blog.csdn.net/rush9838465/article/details/122164791">Blog in Chinese</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Jian Yang, and Christian Poellabauer,&nbsp;<strong>"Detecting Replay Attacks Using Multi-Channel Audio: A Neural Network-Based Method"</strong>, IEEE Signal Processing Letters, 2020.&nbsp;[<a href="https://ieeexplore.ieee.org/document/9099075">Paper</a>][<a href="https://github.com/YuanGongND/multichannel-antispoof">Code</a>]</li>
<li>Bryan Xia, <span style="text-decoration: underline;">Yuan Gong</span>, Yizhe Zhang, and Christian Poellabauer,<strong> "Second-order Non-local Attention Networks for Person Re-identification"</strong>, Proceedings of the 2019 International Conference on Computer Vision (ICCV), Seoul, Korea, October-November 2019. [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.pdf">Paper</a>][<a href="https://blog.csdn.net/weixin_38208912/article/details/104419781">Blog</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Jian Yang, Jacob Huber, Mitchell MacKnight, Christian Poellabauer, <strong>"ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems"</strong>, Proceedings of the 20th Conference of the International Speech Communication Association (Interspeech 2019), Graz, Austria, September 2019 (<span style="color: #ff0000;"><em>best student paper award nomination</em></span>). [<a href="https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/YuanGongND/ReMASC" target="_blank" rel="noopener">Dataset</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Boyang Li, Christian Poellabauer, and Yiyu Shi, <strong>"Real-time Adversarial Attacks"</strong>, Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), Macao, China, August 2019. [<a href="https://www.ijcai.org/proceedings/2019/649" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/YuanGongND/realtime-adversarial-attack">Code</a>][<a href="https://medium.com/ai%C2%B3-theory-practice-business/how-about-real-time-adversarial-attacks-6aba92d59c1e">Media</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer, <strong>"</strong><strong>Deep Obfuscation: Precise Masking of Sensitive Information to Protect Against Machine Learning Adversaries (Poster)"</strong>, Proceedings of the 2018 Annual Computer Security Applications Conference Poster Session, San Juan, Puerto Rico, December 2018.</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer, <strong>"Crafting Adversarial Examples For Speech Paralinguistics Applications"</strong>, Proceedings of the DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security (DYNAMICS) Workshop, San Juan, Puerto Rico, December 2018. [<a href="https://arxiv.org/pdf/1711.03280.pdf">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer,&nbsp;<strong>"Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models"</strong>, Proceedings of the 19th Conference of the International Speech Communication Association (Interspeech 2018), Hyderabad, India, September 2018. [<a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1371.pdf">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Kevin Shin, and Christian Poellabauer,&nbsp;<strong>"Improving LIWC Using Soft Word Matching (Poster)"</strong>, Proceedings of the 9th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB), Washington, DC, August-September 2018. [<a href="https://dl.acm.org/authorize?N667629">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Hasini Yatawatte, Christian Poellabauer, Sandra Schneider, and Susan Latham,&nbsp;<strong>"Automatic Autism Spectrum Disorder Detection Using Everyday Vocalizations Captured by Smart Devices"</strong>, Proceedings of the 9th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB), Washington, DC, August-September 2018.&nbsp; </strong>[<a href="https://dl.acm.org/authorize?N667607">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer,&nbsp;<strong>"Protecting Voice Controlled Systems Using Sound Source Identification Based on Acoustic Cues"</strong>, Proceedings of the 27th International Conference on Computer Communications and Networks (ICCCN), Hangzhou, China, July-August 2018.&nbsp;[<a href="https://ieeexplore.ieee.org/document/8487334/authors#authors" target="_blank" rel="noopener noreferrer">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer,&nbsp;<strong>"An Overview of Vulnerabilities of Voice Controlled Systems"</strong>, Proceedings of the 1st International Workshop on Security and Privacy for the Internet-of-Things (IoTSec), Orlando, FL, April 2018.&nbsp;[<a href="https://arxiv.org/pdf/1803.09156.pdf" target="_blank" rel="noopener noreferrer">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer, "<strong>Topic Modeling Based Multi-modal Depression Detection</strong>", Proceedings of the 7th Audio/Visual Emotion Challenge and Workshop (AVEC) in conjunction with ACM Multimedia (ACM-MM), Mountain View, CA, October 2017 (<span style="color: #ff0000;"><em>depression challenge winner</em></span>). [<a href="https://arxiv.org/pdf/1803.10384.pdf" target="_blank" rel="noopener noreferrer">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span> and Christian Poellabauer,&nbsp;<strong>"Continuous Assessment of Children's Emotional States using Acoustic Analysis"</strong>, Proceedings of the 5th IEEE International Conference on Healthcare Informatics (ICHI), Park City, UT, August 2017. [<a href="https://ieeexplore.ieee.org/document/8031145/" target="_blank" rel="noopener noreferrer">Paper</a>]</li>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Jin Cao, Zehui Luo, Guohui Zhou. <strong>"基于 MSP430F5529 及 CC2540 的智能型低功耗心电监测仪"</strong> (A Smart Low-Power-Consumption ECG Monitor Based on MSP430F5529 and CC2540), Chinese Journal of medical instrumentation 39.4, 2015 (<span style="color: #ff0000;"><em>project won 2014 TI national biomedical device design contest</em></span>). [<a href="http://www.cqvip.com/qk/93920x/201504/665744394.html" target="_blank" rel="noopener noreferrer">Paper</a>]&nbsp;</li>
</ul>
<h3>Preprint</h3>
<hr />
<ul>
<li><span style="text-decoration: underline;">Yuan Gong</span>, Sameer Khurana, Andrew Rouditchenko, and James Glass,&nbsp;<strong>"CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification"</strong>. [<a href="https://arxiv.org/abs/2203.06760">Paper</a>]
</ul>
<h3>Awards</h3>
<hr />
<ul>
<li>INTERSPEECH 2019 Best Student Paper Award Nomination</li>
<li><a href="http://sspnet.eu/avec2017/">Depression Detection Challenge Winner</a>, the 7th ACM Multimedia Audio/Visual Emotion Challenge and Workshop (AVEC 2017)</li>
<li>IJCAI, ISCA, ICHI, NSF Travel Grant</li>
<li>Outstanding Graduate of Fudan University (2015), Fudan First Prize Scholarship (Top 3%, 2014),&nbsp;&nbsp;Outstanding Student of Dept. of Information Technology (2013),&nbsp;&nbsp;Outstanding Student of Fudan University (2012)</li>
</ul>
<h3>Invited Talks and Guest Lectures</h3>
<hr />
<ul>
<li>Introduction of Audio Spectrogram Transformer - Architecture, Training, and Pre-training. <i>Mitsubishi Electric Research Laboratories,</i> 6/8/2022; <i>ByteDance,</i> 6/14/2022; <i>Adobe,</i> 7/12/2022. </li>
<li>Introduction of Audio Spectrogram Transformer - Architecture, Training, and Pre-training. <i>AI Time.</i> 5/26/2022. [<a href="https://www.bilibili.com/video/BV1Va411j73F?spm_id_from=333.999.0.0">video in Mandarin</a>][<a href="https://drive.google.com/file/d/15LWH0QRHkfPWtCxQJ-qdrKOJ96LKEj0c/view?usp=sharing">slides</a>] </li>
<li>General Audio Processing. <i>MIT 6.345/HST.728 Spoken Language Processing (Guest Lecture, Sole Instructor).</i> 4/19/2022. </li>
<li>Audio Spectrogram Transformer. <i>MIT Embodied Intelligence Seminar.</i> 10/14/2021. [<a href="https://youtu.be/wcVejqmb1mQ?t=1556">video</a>] </li>
<li>Audio Spectrogram Transformer for Audio Scene Analysis. <i>ISCA SIGML Seminar.</i> 6/16/2021. [<a href="https://youtu.be/CSRDbqGY0Vw">video</a>][<a href="https://drive.google.com/file/d/1mKktIq5OAltcIXwI45vLRGqx3VxjLSn7/view?usp=sharing">slides</a>] </li>
<li>Win the cat and mouse game: ensuring the security of the speech processing systems to real world threats. <i>University of Notre Dame CSE60641 (Guest Lecture, Sole Instructor).</i> 10/31/2019. [<a href="https://drive.google.com/file/d/1EMRhJR0dv1NSj8MS55nHNVD7z7waCq7e/view?usp=sharing">slides</a>] </li>
<li>Speech Processing: Machine Learning Approaches, Novel Applications, and New Security Concerns. <i>University of Notre Dame CSE60641 (Guest Lecture, Sole Instructor).</i> 9/20/2018. [<a href="https://drive.google.com/file/d/1mKktIq5OAltcIXwI45vLRGqx3VxjLSn7/view?usp=sharing">slides</a>] </li>
</ul>
<h3>Contact</h3>
<hr />
<p>Please feel free to reach out (yuangong@mit.edu) if you have any questions about my work. I do not use WeChat. As a postdoc, I am not involved in student/researcher recruitment at CSAIL, please contact a PI for such inquiries.</p>
* Nearly all my papers are incorrectly linked to a <a href="https://scholar.google.com/citations?user=8BX2aRoAAAAJ&hl=en&oi=sra">UESTC Professor Yuan Gong</a> due to a bug in Google Scholar's system. That is not me, I have never worked at UESTC. 
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=rra7QS5zkujIOvkpex4FoZ6CWhS0vZIMatAVAMCJnvU'></script>
<p>&nbsp;</p>
